{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Assignment 3: Retrieval-Augmented Generation (RAG) using LangChain\n", "\n", "## Part I: Conceptual Understanding\n", "### Q1: Motivation behind Retrieval-Augmented Generation (RAG)\n", "RAG helps enhance LLMs by retrieving relevant external documents during inference, leading to more accurate and grounded answers.\n", "\n", "### Q2: RAG vs Standard LLM QA\n", "Standard LLM QA relies on pre-trained internal knowledge. RAG augments this with external documents retrieved at runtime, reducing hallucinations.\n", "\n", "### Q3: Role of Vector Store\n", "It stores vector representations of documents and enables retrieval of relevant chunks based on similarity to user queries.\n", "\n", "### Q4: Chain Types in LangChain\n", "- **stuff**: Concatenates all chunks.\n", "- **map_reduce**: Answers each chunk, then combines results.\n", "- **refine**: Builds the answer incrementally from each chunk.\n", "\n", "### Q5: Main Components of a LangChain RAG Pipeline\n", "- Document Loader\n", "- Text Splitter\n", "- Embeddings\n", "- Vector Store\n", "- Retriever\n", "- LLM\n", "- RetrievalQA Chain\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Part II: RAG Implementation with LangChain"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Install required packages (uncomment and run if needed)\n", "# !pip install langchain chromadb pypdf tqdm ollama\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from langchain.document_loaders import PyPDFLoader\n", "from langchain.text_splitter import RecursiveCharacterTextSplitter\n", "from langchain.embeddings import OllamaEmbeddings\n", "from langchain.vectorstores import Chroma\n", "from langchain.llms import Ollama\n", "from langchain.chains import RetrievalQA\n", "\n", "# Load document\n", "loader = PyPDFLoader(\"data/test_doc.pdf\")\n", "pages = loader.load()\n", "\n", "# Split document into chunks\n", "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n", "docs = splitter.split_documents(pages)\n", "\n", "# Create embeddings and vector store\n", "embeddings = OllamaEmbeddings(model=\"mistral\")\n", "vectorstore = Chroma.from_documents(docs, embeddings, persist_directory=\".chromadb\")\n", "vectorstore.persist()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Initialize retriever and LLM\n", "retriever = vectorstore.as_retriever()\n", "llm = Ollama(model=\"mistral\")\n", "\n", "# Define RAG pipeline\n", "qa_chain = RetrievalQA.from_chain_type(\n", "    llm=llm,\n", "    retriever=retriever,\n", "    return_source_documents=True\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Sample queries\n", "queries = [\n", "    \"What is the main topic of the document?\",\n", "    \"List any key benefits mentioned.\",\n", "    \"What problem does this paper solve?\",\n", "    \"Give a summary in bullet points.\",\n", "    \"Are there any limitations discussed?\"\n", "]\n", "\n", "for q in queries:\n", "    result = qa_chain(q)\n", "    print(f\"\\n\\033[1mQuestion:\\033[0m {q}\")\n", "    print(f\"\\033[94mAnswer:\\033[0m {result['result']}\")\n", "    print(\"\\033[92mRetrieved Sources:\\033[0m\")\n", "    for doc in result['source_documents']:\n", "        print(\"-\", doc.metadata.get(\"source\"))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Custom Prompt Template\n", "- Added citation tags like [source]\n", "- Disclaimer: \"Answer is based on retrieved document context\"\n", "- Output formatted as bullet points"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}